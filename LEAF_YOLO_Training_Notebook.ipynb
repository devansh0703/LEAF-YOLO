{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8610e3d6",
   "metadata": {},
   "source": [
    "# LEAF-YOLO: Lightweight Aerial Small Object Detection\n",
    "\n",
    "This notebook implements the complete LEAF-YOLO architecture for efficient small object detection in aerial imagery, specifically designed for the VisDrone dataset. LEAF-YOLO introduces novel lightweight mechanisms while maintaining high accuracy for edge deployment on UAV platforms.\n",
    "\n",
    "## Model Specifications:\n",
    "- **LEAF-YOLO-N**: 1.2M parameters, 5.6G FLOPs, 56 FPS on Jetson AGX Xavier\n",
    "- **LEAF-YOLO**: 4.28M parameters, 20.9G FLOPs, 32 FPS on Jetson AGX Xavier\n",
    "- **Target Dataset**: VisDrone2019-DET with 10 classes\n",
    "- **Performance**: 28.2% mAP@50:95, 48.3% mAP@50 on VisDrone validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf1384",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for building and training the LEAF-YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Numerical and scientific computing\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Visualization and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6fb01",
   "metadata": {},
   "source": [
    "## 2. Define Core Modules (GhostConv, PConv, CoordConvATT)\n",
    "\n",
    "Implement the fundamental building blocks of LEAF-YOLO architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with batch normalization and activation\"\"\"\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "def autopad(k, p=None):\n",
    "    \"\"\"Auto-pad to maintain same dimensions\"\"\"\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
    "    return p\n",
    "\n",
    "class GhostConv(nn.Module):\n",
    "    \"\"\"Ghost Convolution - generates more features from fewer parameters\"\"\"\n",
    "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):\n",
    "        super().__init__()\n",
    "        c_ = c2 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k, s, None, g, act)\n",
    "        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.cv1(x)\n",
    "        return torch.cat([y, self.cv2(y)], 1)\n",
    "\n",
    "class PConv(nn.Module):\n",
    "    \"\"\"Partial Convolution - applies convolution to partial channels\"\"\"\n",
    "    def __init__(self, c1, c2=None, k=3, s=1, p=1, g=1):\n",
    "        super().__init__()\n",
    "        c2 = c2 or c1\n",
    "        self.dim = c1 // 4  # Use 1/4 of channels for convolution\n",
    "        self.conv = nn.Conv2d(self.dim, self.dim, k, s, p, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(self.dim)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = torch.split(x, [self.dim, x.shape[1] - self.dim], dim=1)\n",
    "        x1 = self.act(self.bn(self.conv(x1)))\n",
    "        return torch.cat([x1, x2], dim=1)\n",
    "\n",
    "class ECA(nn.Module):\n",
    "    \"\"\"Efficient Channel Attention\"\"\"\n",
    "    def __init__(self, channel, b=1, gamma=2):\n",
    "        super().__init__()\n",
    "        t = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        k = t if t % 2 else t + 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2))\n",
    "        y = y.transpose(-1, -2).unsqueeze(-1)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y\n",
    "\n",
    "class CoordAtt(nn.Module):\n",
    "    \"\"\"Coordinate Attention mechanism\"\"\"\n",
    "    def __init__(self, inp, oup, reduction=32):\n",
    "        super().__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // reduction)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.size()\n",
    "        \n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "        \n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "        \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "        \n",
    "        a_h = self.conv_h(x_h).sigmoid()\n",
    "        a_w = self.conv_w(x_w).sigmoid()\n",
    "        \n",
    "        out = x * a_w * a_h\n",
    "        \n",
    "        return out\n",
    "\n",
    "class CoordConvATT(nn.Module):\n",
    "    \"\"\"Coordinate Convolution with Attention\"\"\"\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(c1, c2, k, s, p, g, act)\n",
    "        self.coord_att = CoordAtt(c2, c2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.coord_att(x)\n",
    "        return x\n",
    "\n",
    "print(\"Core modules defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2eecf3",
   "metadata": {},
   "source": [
    "## 3. Implement LEAF Blocks and CSP Components\n",
    "\n",
    "Create the LEAF mechanism and Cross-Stage Partial (CSP) blocks with Bottle2neck modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottle2neck(nn.Module):\n",
    "    \"\"\"Res2Net Bottleneck block with multiple scales\"\"\"\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5, scale=4, stype='normal'):\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.scale = scale\n",
    "        self.nums = scale - 1\n",
    "        \n",
    "        self.conv1 = Conv(c1, c_, 1, 1)\n",
    "        \n",
    "        # Multiple scale convolutions\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(Conv(c_ // scale, c_ // scale, 3, 1))\n",
    "            \n",
    "        self.conv2 = Conv(c_, c2, 1, 1, act=False)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        spx = torch.split(out, out.size(1) // self.scale, 1)\n",
    "        sp_outs = [spx[0]]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            sp = spx[i + 1] + sp_outs[i] if i >= 1 else spx[i + 1]\n",
    "            sp_out = conv(sp)\n",
    "            sp_outs.append(sp_out)\n",
    "            \n",
    "        out = torch.cat(sp_outs, 1)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return out + residual if self.add else out\n",
    "\n",
    "class C3_Res2Block(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with Res2Net blocks\"\"\"\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)\n",
    "        \n",
    "        self.m = nn.Sequential(*(Bottle2neck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n",
    "\n",
    "class LEAFBlock(nn.Module):\n",
    "    \"\"\"Lightweight-Efficient Aggregating Fusion (LEAF) Block\"\"\"\n",
    "    def __init__(self, c1, c2, n_pconv=4):\n",
    "        super().__init__()\n",
    "        self.c_split = c1 // 2\n",
    "        \n",
    "        # Split into two paths\n",
    "        self.cv1 = Conv(self.c_split, self.c_split, 1, 1)\n",
    "        self.cv2 = Conv(self.c_split, self.c_split, 1, 1)\n",
    "        \n",
    "        # Multiple PConv layers for feature aggregation\n",
    "        self.pconvs = nn.ModuleList([PConv(self.c_split) for _ in range(n_pconv)])\n",
    "        \n",
    "        # Final fusion\n",
    "        self.cv_out = Conv(c1 + self.c_split * n_pconv, c2, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input\n",
    "        x1, x2 = torch.split(x, [self.c_split, x.shape[1] - self.c_split], dim=1)\n",
    "        \n",
    "        # Process first path\n",
    "        y1 = self.cv1(x1)\n",
    "        \n",
    "        # Process second path with multiple PConv\n",
    "        y2 = self.cv2(x2)\n",
    "        pconv_outputs = [y2]\n",
    "        \n",
    "        for pconv in self.pconvs:\n",
    "            y2 = pconv(y2)\n",
    "            pconv_outputs.append(y2)\n",
    "            \n",
    "        # Concatenate all outputs\n",
    "        out = torch.cat([x1, y1] + pconv_outputs, dim=1)\n",
    "        return self.cv_out(out)\n",
    "\n",
    "class MP(nn.Module):\n",
    "    \"\"\"MaxPool wrapper\"\"\"\n",
    "    def __init__(self, k=2):\n",
    "        super().__init__()\n",
    "        self.m = nn.MaxPool2d(kernel_size=k, stride=k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x)\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    \"\"\"Concatenate tensors along dimension\"\"\"\n",
    "    def __init__(self, dimension=1):\n",
    "        super().__init__()\n",
    "        self.d = dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(x, self.d)\n",
    "\n",
    "print(\"LEAF blocks and CSP components implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2805f6c",
   "metadata": {},
   "source": [
    "## 4. Build Backbone Architecture\n",
    "\n",
    "Construct the hierarchical backbone with progressive feature extraction through 5 stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c35c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEAFBackbone(nn.Module):\n",
    "    \"\"\"LEAF-YOLO Backbone with 5 stages of feature extraction\"\"\"\n",
    "    def __init__(self, width_multiple=0.5, depth_multiple=1.0):\n",
    "        super().__init__()\n",
    "        self.width_multiple = width_multiple\n",
    "        self.depth_multiple = depth_multiple\n",
    "        \n",
    "        # Calculate channel numbers based on width_multiple\n",
    "        def make_divisible(x, divisor=8):\n",
    "            return math.ceil(x / divisor) * divisor\n",
    "        \n",
    "        # Channel configuration for nano version\n",
    "        channels = [32, 64, 128, 256, 256]\n",
    "        channels = [make_divisible(c * width_multiple) for c in channels]\n",
    "        \n",
    "        # Stage 0: Input processing (640x640 -> 320x320)\n",
    "        self.stem = Conv(3, channels[0], 3, 2)  # P1/2\n",
    "        \n",
    "        # Stage 1: P2 (320x320 -> 160x160) \n",
    "        self.stage1 = nn.Sequential(\n",
    "            Conv(channels[0], channels[1], 3, 2),  # P2/4\n",
    "            GhostConv(channels[1], channels[1], 3, 1),\n",
    "            LEAFBlock(channels[1], channels[1]),\n",
    "            C3_Res2Block(channels[1], channels[1])\n",
    "        )\n",
    "        \n",
    "        # Stage 2: P3 (160x160 -> 80x80)\n",
    "        self.stage2_down = nn.Sequential(\n",
    "            MP(2),\n",
    "            Conv(channels[1], channels[1], 1, 1),\n",
    "        )\n",
    "        self.stage2_ghost = nn.Sequential(\n",
    "            Conv(channels[1], channels[1], 1, 1),\n",
    "            GhostConv(channels[1], channels[1], 3, 2),\n",
    "        )\n",
    "        self.stage2_leaf = LEAFBlock(channels[1] * 2, channels[1])\n",
    "        self.stage2_c3 = C3_Res2Block(channels[1], channels[2])\n",
    "        \n",
    "        # Stage 3: P4 (80x80 -> 40x40)\n",
    "        self.stage3_down = nn.Sequential(\n",
    "            MP(2),\n",
    "            Conv(channels[2], channels[2], 1, 1),\n",
    "        )\n",
    "        self.stage3_ghost = nn.Sequential(\n",
    "            Conv(channels[2], channels[2], 1, 1),\n",
    "            GhostConv(channels[2], channels[2], 3, 2),\n",
    "        )\n",
    "        self.stage3_leaf = LEAFBlock(channels[2] * 2, channels[2])\n",
    "        self.stage3_c3 = C3_Res2Block(channels[2], channels[3])\n",
    "        \n",
    "        # Stage 4: P5 (40x40 -> 20x20)\n",
    "        self.stage4_down = nn.Sequential(\n",
    "            MP(2),\n",
    "            Conv(channels[3], channels[3], 1, 1),\n",
    "        )\n",
    "        self.stage4_ghost = nn.Sequential(\n",
    "            Conv(channels[3], channels[3], 1, 1),\n",
    "            GhostConv(channels[3], channels[3], 3, 2),\n",
    "        )\n",
    "        self.stage4_leaf = LEAFBlock(channels[3] * 2, channels[3])\n",
    "        self.stage4_c3 = C3_Res2Block(channels[3], channels[4])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        \n",
    "        # Stem: 640x640 -> 320x320\n",
    "        x = self.stem(x)  # P1\n",
    "        \n",
    "        # Stage 1: 320x320 -> 160x160\n",
    "        x = self.stage1(x)  # P2\n",
    "        outputs.append(x)\n",
    "        \n",
    "        # Stage 2: 160x160 -> 80x80  \n",
    "        x1 = self.stage2_down(x)\n",
    "        x2 = self.stage2_ghost(x)\n",
    "        x = torch.cat([x1, x2], dim=1)  # Concat\n",
    "        x = self.stage2_leaf(x)\n",
    "        x = self.stage2_c3(x)  # P3\n",
    "        outputs.append(x)\n",
    "        \n",
    "        # Stage 3: 80x80 -> 40x40\n",
    "        x1 = self.stage3_down(x)\n",
    "        x2 = self.stage3_ghost(x)\n",
    "        x = torch.cat([x1, x2], dim=1)  # Concat\n",
    "        x = self.stage3_leaf(x)\n",
    "        x = self.stage3_c3(x)  # P4\n",
    "        outputs.append(x)\n",
    "        \n",
    "        # Stage 4: 40x40 -> 20x20\n",
    "        x1 = self.stage4_down(x)\n",
    "        x2 = self.stage4_ghost(x)\n",
    "        x = torch.cat([x1, x2], dim=1)  # Concat\n",
    "        x = self.stage4_leaf(x)\n",
    "        x = self.stage4_c3(x)  # P5\n",
    "        outputs.append(x)\n",
    "        \n",
    "        return outputs  # [P2, P3, P4, P5]\n",
    "\n",
    "# Test backbone\n",
    "print(\"Testing LEAF Backbone...\")\n",
    "backbone = LEAFBackbone(width_multiple=0.5)\n",
    "test_input = torch.randn(1, 3, 640, 640)\n",
    "features = backbone(test_input)\n",
    "\n",
    "print(\"Backbone output shapes:\")\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"P{i+2}: {feat.shape}\")\n",
    "    \n",
    "print(\"Backbone implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c005b2",
   "metadata": {},
   "source": [
    "## 5. Create Neck (Feature Pyramid Network)\n",
    "\n",
    "Implement the PANet-style neck with SPPRFEM and bidirectional feature fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaec36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPRFEM(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling with Receptive Field Enhancement Module\"\"\"\n",
    "    def __init__(self, c1, c2, k=(5, 9, 13)):\n",
    "        super().__init__()\n",
    "        c_ = c1 // 2  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n",
    "        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n",
    "\n",
    "class LEAFNeck(nn.Module):\n",
    "    \"\"\"LEAF-YOLO Neck with PANet-style feature fusion\"\"\"\n",
    "    def __init__(self, channels=[64, 128, 256, 256]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # SPPF for P5\n",
    "        self.sppf = SPPRFEM(channels[3], channels[2])\n",
    "        \n",
    "        # Top-down path (P5 -> P4 -> P3 -> P2)\n",
    "        self.up_conv1 = CoordConvATT(channels[2], channels[1], 1, 1)\n",
    "        self.up_conv2 = CoordConvATT(channels[1], channels[0], 1, 1)  \n",
    "        self.up_conv3 = CoordConvATT(channels[0], channels[0], 1, 1)\n",
    "        \n",
    "        # Upsample layers\n",
    "        self.upsample = nn.Upsample(None, 2, 'nearest')\n",
    "        \n",
    "        # Fusion blocks after concatenation\n",
    "        self.fusion1 = C3_Res2Block(channels[2] + channels[1], channels[1])  # P4 fusion\n",
    "        self.fusion2 = C3_Res2Block(channels[1] + channels[0], channels[0])  # P3 fusion  \n",
    "        self.fusion3 = C3_Res2Block(channels[0] * 2, channels[0])  # P2 fusion\n",
    "        \n",
    "        # Bottom-up path (P2 -> P3 -> P4 -> P5)\n",
    "        self.down_conv1 = GhostConv(channels[0], channels[1], 3, 2)\n",
    "        self.down_conv2 = GhostConv(channels[1], channels[2], 3, 2)\n",
    "        self.down_conv3 = GhostConv(channels[2], channels[1], 3, 2)\n",
    "        \n",
    "        # Final fusion blocks\n",
    "        self.final_fusion1 = C3_Res2Block(channels[0] + channels[1], channels[1])  # P3 final\n",
    "        self.final_fusion2 = C3_Res2Block(channels[1] + channels[2], channels[2])  # P4 final\n",
    "        self.final_fusion3 = C3_Res2Block(channels[2] + channels[1], channels[2])  # P5 final\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [P2, P3, P4, P5] from backbone\n",
    "        Returns:\n",
    "            [P2_out, P3_out, P4_out, P5_out] for detection heads\n",
    "        \"\"\"\n",
    "        p2, p3, p4, p5 = features\n",
    "        \n",
    "        # Apply SPPF to P5\n",
    "        p5_spp = self.sppf(p5)\n",
    "        \n",
    "        # Top-down path\n",
    "        # P5 -> P4\n",
    "        p5_up = self.up_conv1(p5_spp)\n",
    "        p5_up = self.upsample(p5_up)\n",
    "        p4_fused = torch.cat([p5_up, p4], dim=1)\n",
    "        p4_fused = self.fusion1(p4_fused)\n",
    "        \n",
    "        # P4 -> P3\n",
    "        p4_up = self.up_conv2(p4_fused)\n",
    "        p4_up = self.upsample(p4_up)\n",
    "        p3_fused = torch.cat([p4_up, p3], dim=1)\n",
    "        p3_fused = self.fusion2(p3_fused)\n",
    "        \n",
    "        # P3 -> P2\n",
    "        p3_up = self.up_conv3(p3_fused)\n",
    "        p3_up = self.upsample(p3_up)\n",
    "        p2_fused = torch.cat([p3_up, p2], dim=1)\n",
    "        p2_out = self.fusion3(p2_fused)\n",
    "        \n",
    "        # Bottom-up path\n",
    "        # P2 -> P3\n",
    "        p2_down = self.down_conv1(p2_out)\n",
    "        p3_final = torch.cat([p2_down, p3_fused], dim=1)\n",
    "        p3_out = self.final_fusion1(p3_final)\n",
    "        \n",
    "        # P3 -> P4\n",
    "        p3_down = self.down_conv2(p3_out)\n",
    "        p4_final = torch.cat([p3_down, p4_fused], dim=1)\n",
    "        p4_out = self.final_fusion2(p4_final)\n",
    "        \n",
    "        # P4 -> P5\n",
    "        p4_down = self.down_conv3(p4_out)\n",
    "        p5_final = torch.cat([p4_down, p5_spp], dim=1)\n",
    "        p5_out = self.final_fusion3(p5_final)\n",
    "        \n",
    "        return [p2_out, p3_out, p4_out, p5_out]\n",
    "\n",
    "# Test neck\n",
    "print(\"Testing LEAF Neck...\")\n",
    "neck = LEAFNeck([64, 128, 256, 256])\n",
    "neck_outputs = neck(features)\n",
    "\n",
    "print(\"Neck output shapes:\")\n",
    "for i, feat in enumerate(neck_outputs):\n",
    "    print(f\"P{i+2}_out: {feat.shape}\")\n",
    "    \n",
    "print(\"Neck implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bcf53",
   "metadata": {},
   "source": [
    "## 6. Implement Detection Head (IDetect)\n",
    "\n",
    "Build the improved detection head with implicit layers for multi-scale detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDetect(nn.Module):\n",
    "    \"\"\"Improved Detection Head with implicit layers and anchor-based detection\"\"\"\n",
    "    def __init__(self, nc=10, anchors=(), ch=(), inplace=True):\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.no = nc + 5  # number of outputs per anchor\n",
    "        self.nl = len(anchors)  # number of detection layers\n",
    "        self.na = len(anchors[0]) // 2  # number of anchors\n",
    "        self.grid = [torch.zeros(1)] * self.nl  # init grid\n",
    "        self.anchor_grid = [torch.zeros(1)] * self.nl  # init anchor grid\n",
    "        \n",
    "        # Register anchors\n",
    "        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))\n",
    "        \n",
    "        # Detection convolutions\n",
    "        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)\n",
    "        \n",
    "        # Implicit layers for knowledge distillation\n",
    "        self.ia = nn.ModuleList(nn.Parameter(torch.zeros(1, self.na, 1, 1)) for _ in range(self.nl))\n",
    "        self.im = nn.ModuleList(nn.Parameter(torch.ones(1, self.no * self.na, 1, 1)) for _ in range(self.nl))\n",
    "        \n",
    "        self.inplace = inplace\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = []  # inference output\n",
    "        \n",
    "        for i in range(self.nl):\n",
    "            # Apply implicit layers\n",
    "            x[i] = x[i] + self.ia[i]\n",
    "            x[i] = self.m[i](x[i])  # conv\n",
    "            x[i] = x[i] * self.im[i]  # implicit multiplication\n",
    "            \n",
    "            bs, _, ny, nx = x[i].shape\n",
    "            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "            \n",
    "            if not self.training:  # inference\n",
    "                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
    "                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n",
    "                \n",
    "                y = x[i].sigmoid()\n",
    "                if self.inplace:\n",
    "                    y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
    "                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n",
    "                    xy = (y[..., 0:2] * 2 - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
    "                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
    "                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n",
    "                z.append(y.view(bs, -1, self.no))\n",
    "                \n",
    "        return x if self.training else (torch.cat(z, 1), x)\n",
    "    \n",
    "    def _make_grid(self, nx=20, ny=20, i=0):\n",
    "        d = self.anchors[i].device\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny).to(d), torch.arange(nx).to(d)], indexing='ij')\n",
    "        grid = torch.stack((xv, yv), 2).expand((1, self.na, ny, nx, 2)).float()\n",
    "        anchor_grid = (self.anchors[i].clone() * self.stride[i]).view((1, self.na, 1, 1, 2)).expand((1, self.na, ny, nx, 2)).float()\n",
    "        return grid, anchor_grid\n",
    "\n",
    "class LEAFDetectionHead(nn.Module):\n",
    "    \"\"\"Complete LEAF-YOLO Detection Head with PConv preprocessing\"\"\"\n",
    "    def __init__(self, nc=10, anchors=(), ch=(), head_channels=[256, 256, 512, 512]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Preprocess features with PConv before detection\n",
    "        self.preprocess = nn.ModuleList([\n",
    "            PConv(ch[i], head_channels[i]) for i in range(len(ch))\n",
    "        ])\n",
    "        \n",
    "        # Main detection head\n",
    "        self.detect = IDetect(nc, anchors, head_channels)\n",
    "        \n",
    "        # Initialize stride\n",
    "        self.stride = torch.tensor([4., 8., 16., 32.])  # P2, P3, P4, P5 strides\n",
    "        self.detect.stride = self.stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Preprocess each feature map\n",
    "        processed = []\n",
    "        for i, feat in enumerate(x):\n",
    "            processed.append(self.preprocess[i](feat))\n",
    "            \n",
    "        return self.detect(processed)\n",
    "\n",
    "# VisDrone anchors (optimized for small objects)\n",
    "visdrone_anchors = [\n",
    "    [2.9434, 4.0435, 3.8626, 8.5592, 6.8534, 5.9391],  # P2/4\n",
    "    [10, 13, 16, 30, 33, 23],                           # P3/8\n",
    "    [30, 61, 62, 45, 59, 119],                          # P4/16\n",
    "    [116, 90, 156, 198, 373, 326]                       # P5/32\n",
    "]\n",
    "\n",
    "# Test detection head\n",
    "print(\"Testing LEAF Detection Head...\")\n",
    "head = LEAFDetectionHead(nc=10, anchors=visdrone_anchors, ch=[64, 128, 256, 256])\n",
    "head.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    detection_outputs = head(neck_outputs)\n",
    "    \n",
    "if isinstance(detection_outputs, tuple):\n",
    "    inference_output, training_output = detection_outputs\n",
    "    print(f\"Inference output shape: {inference_output.shape}\")\n",
    "    print(f\"Training outputs: {[t.shape for t in training_output]}\")\n",
    "else:\n",
    "    print(f\"Training output shapes: {[t.shape for t in detection_outputs]}\")\n",
    "    \n",
    "print(\"Detection head implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f66d6",
   "metadata": {},
   "source": [
    "## 7. Assemble Complete LEAF-YOLO Model\n",
    "\n",
    "Combine all components into the final LEAF-YOLO architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEAFYOLO(nn.Module):\n",
    "    \"\"\"Complete LEAF-YOLO Model for Aerial Small Object Detection\"\"\"\n",
    "    def __init__(self, nc=10, width_multiple=0.5, depth_multiple=1.0):\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        self.width_multiple = width_multiple\n",
    "        self.depth_multiple = depth_multiple\n",
    "        \n",
    "        # Calculate channels based on width_multiple\n",
    "        def make_divisible(x, divisor=8):\n",
    "            return math.ceil(x / divisor) * divisor\n",
    "        \n",
    "        if width_multiple == 0.5:  # Nano version\n",
    "            channels = [32, 64, 128, 256, 256]\n",
    "        else:  # Standard version\n",
    "            channels = [32, 64, 128, 256, 512]\n",
    "            \n",
    "        channels = [make_divisible(c * width_multiple) for c in channels]\n",
    "        \n",
    "        # Model components\n",
    "        self.backbone = LEAFBackbone(width_multiple, depth_multiple)\n",
    "        self.neck = LEAFNeck(channels)\n",
    "        self.head = LEAFDetectionHead(\n",
    "            nc=nc, \n",
    "            anchors=visdrone_anchors, \n",
    "            ch=channels,\n",
    "            head_channels=[256, 256, 512, 512] if width_multiple == 0.5 else [512, 512, 1024, 1024]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features through backbone\n",
    "        backbone_features = self.backbone(x)\n",
    "        \n",
    "        # Enhance features through neck\n",
    "        neck_features = self.neck(backbone_features)\n",
    "        \n",
    "        # Generate detections\n",
    "        detections = self.head(neck_features)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information including parameters and FLOPs\"\"\"\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        def calculate_flops(model, input_size=(1, 3, 640, 640)):\n",
    "            \"\"\"Approximate FLOPs calculation\"\"\"\n",
    "            model.eval()\n",
    "            total_flops = 0\n",
    "            \n",
    "            def flop_count_hook(module, input, output):\n",
    "                nonlocal total_flops\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    batch_size = input[0].shape[0]\n",
    "                    output_dims = output.shape[2:]\n",
    "                    kernel_dims = module.kernel_size\n",
    "                    in_channels = module.in_channels\n",
    "                    out_channels = module.out_channels\n",
    "                    groups = module.groups\n",
    "                    \n",
    "                    filters_per_channel = out_channels // groups\n",
    "                    conv_per_position_flops = int(np.prod(kernel_dims)) * in_channels // groups\n",
    "                    \n",
    "                    active_elements_count = batch_size * int(np.prod(output_dims))\n",
    "                    overall_conv_flops = conv_per_position_flops * active_elements_count * filters_per_channel\n",
    "                    \n",
    "                    total_flops += overall_conv_flops\n",
    "            \n",
    "            hooks = []\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    hooks.append(module.register_forward_hook(flop_count_hook))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model(torch.randn(*input_size))\n",
    "            \n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "                \n",
    "            return total_flops\n",
    "        \n",
    "        params = count_parameters(self)\n",
    "        flops = calculate_flops(self)\n",
    "        \n",
    "        return {\n",
    "            'parameters': params,\n",
    "            'parameters_M': params / 1e6,\n",
    "            'FLOPs': flops,\n",
    "            'FLOPs_G': flops / 1e9,\n",
    "            'width_multiple': self.width_multiple,\n",
    "            'model_type': 'LEAF-YOLO-N' if self.width_multiple == 0.5 else 'LEAF-YOLO'\n",
    "        }\n",
    "\n",
    "# Create model variants\n",
    "print(\"Creating LEAF-YOLO models...\")\n",
    "\n",
    "# LEAF-YOLO-N (Nano)\n",
    "model_nano = LEAFYOLO(nc=10, width_multiple=0.5, depth_multiple=1.0)\n",
    "print(\"LEAF-YOLO-N created!\")\n",
    "\n",
    "# LEAF-YOLO (Standard)  \n",
    "model_standard = LEAFYOLO(nc=10, width_multiple=1.0, depth_multiple=1.0)\n",
    "print(\"LEAF-YOLO Standard created!\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\\\nTesting forward pass...\")\n",
    "test_input = torch.randn(2, 3, 640, 640)  # Batch of 2 images\n",
    "\n",
    "# Test nano model\n",
    "model_nano.eval()\n",
    "with torch.no_grad():\n",
    "    nano_output = model_nano(test_input)\n",
    "    \n",
    "print(f\"Nano model output type: {type(nano_output)}\")\n",
    "if isinstance(nano_output, tuple):\n",
    "    print(f\"Inference output shape: {nano_output[0].shape}\")\n",
    "    print(f\"Training outputs: {len(nano_output[1])} layers\")\n",
    "else:\n",
    "    print(f\"Training output: {len(nano_output)} layers\")\n",
    "\n",
    "# Get model information\n",
    "nano_info = model_nano.get_model_info()\n",
    "standard_info = model_standard.get_model_info()\n",
    "\n",
    "print(\"\\\\nModel Information:\")\n",
    "print(f\"LEAF-YOLO-N: {nano_info['parameters_M']:.2f}M params, {nano_info['FLOPs_G']:.1f}G FLOPs\")\n",
    "print(f\"LEAF-YOLO: {standard_info['parameters_M']:.2f}M params, {standard_info['FLOPs_G']:.1f}G FLOPs\")\n",
    "\n",
    "print(\"\\\\nLEAF-YOLO models assembled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bcb81",
   "metadata": {},
   "source": [
    "## 8. Model Configuration and Training Setup\n",
    "\n",
    "Configure training parameters and loss functions for VisDrone dataset training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration for LEAF-YOLO on VisDrone dataset\"\"\"\n",
    "    def __init__(self):\n",
    "        # Dataset configuration\n",
    "        self.dataset = 'VisDrone2019-DET'\n",
    "        self.nc = 10  # Number of classes\n",
    "        self.class_names = [\n",
    "            'pedestrian', 'people', 'bicycle', 'car', 'van', \n",
    "            'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'\n",
    "        ]\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = 1000\n",
    "        self.batch_size = 16\n",
    "        self.img_size = 640\n",
    "        self.lr0 = 0.01  # Initial learning rate\n",
    "        self.lrf = 0.01  # Final learning rate (lr0 * lrf)\n",
    "        self.momentum = 0.937\n",
    "        self.weight_decay = 0.0005\n",
    "        self.warmup_epochs = 3\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        self.hsv_h = 0.015  # Hue augmentation\n",
    "        self.hsv_s = 0.7    # Saturation augmentation  \n",
    "        self.hsv_v = 0.4    # Value augmentation\n",
    "        self.degrees = 0.0  # Rotation degrees\n",
    "        self.translate = 0.1 # Translation\n",
    "        self.scale = 0.5    # Scale\n",
    "        self.shear = 0.0    # Shear\n",
    "        self.perspective = 0.0 # Perspective\n",
    "        self.flipud = 0.0   # Vertical flip probability\n",
    "        self.fliplr = 0.5   # Horizontal flip probability\n",
    "        self.mosaic = 1.0   # Mosaic probability\n",
    "        self.mixup = 0.0    # MixUp probability\n",
    "        \n",
    "        # Loss weights\n",
    "        self.box_loss_gain = 0.05\n",
    "        self.cls_loss_gain = 0.5\n",
    "        self.obj_loss_gain = 1.0\n",
    "        self.label_smoothing = 0.0\n",
    "        \n",
    "        # Anchor matching\n",
    "        self.anchor_t = 4.0  # Anchor-multiple threshold\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Loss Functions\n",
    "class ComputeLoss:\n",
    "    \"\"\"Compute loss for LEAF-YOLO training\"\"\"\n",
    "    def __init__(self, model, autobalance=False):\n",
    "        self.sort_obj_iou = False\n",
    "        device = next(model.parameters()).device\n",
    "        h = model.head.detect  # Detect() module\n",
    "        \n",
    "        # Define criteria\n",
    "        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device=device))\n",
    "        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device=device))\n",
    "        \n",
    "        # Class label smoothing\n",
    "        self.cp, self.cn = smooth_BCE(eps=config.label_smoothing)\n",
    "        \n",
    "        # Focal loss\n",
    "        g = 1.5  # focal loss gamma\n",
    "        if config.cls_loss_gain > 0:\n",
    "            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)\n",
    "            \n",
    "        det = h  # Detect() module\n",
    "        self.balance = [4.0, 1.0, 0.4]  # P3-P5\n",
    "        self.ssi = list(det.stride).index(16) if autobalance else 0  # stride 16 index\n",
    "        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, config, autobalance\n",
    "        \n",
    "        for k in 'na', 'nc', 'nl', 'anchors':\n",
    "            setattr(self, k, getattr(det, k))\n",
    "            \n",
    "    def __call__(self, p, targets):\n",
    "        device = targets.device\n",
    "        lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n",
    "        tcls, tbox, indices, anchors = self.build_targets(p, targets)\n",
    "        \n",
    "        # Losses\n",
    "        for i, pi in enumerate(p):  # layer index, layer predictions\n",
    "            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n",
    "            tobj = torch.zeros_like(pi[..., 0], device=device)  # target obj\n",
    "            \n",
    "            n = b.shape[0]  # number of targets\n",
    "            if n:\n",
    "                ps = pi[b, a, gj, gi]  # prediction subset corresponding to targets\n",
    "                \n",
    "                # Regression\n",
    "                pxy = ps[:, :2].sigmoid() * 2 - 0.5\n",
    "                pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]\n",
    "                pbox = torch.cat((pxy, pwh), 1)  # predicted box\n",
    "                iou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=False, CIoU=True)  # iou(prediction, target)\n",
    "                lbox += (1.0 - iou).mean()  # iou loss\n",
    "                \n",
    "                # Objectness\n",
    "                iou = iou.detach().clamp(0).type(tobj.dtype)\n",
    "                if self.sort_obj_iou:\n",
    "                    sort_id = torch.argsort(iou)\n",
    "                    b, a, gj, gi, iou = b[sort_id], a[sort_id], gj[sort_id], gi[sort_id], iou[sort_id]\n",
    "                tobj[b, a, gj, gi] = (1.0 - self.gr) + self.gr * iou  # iou ratio\n",
    "                \n",
    "                # Classification\n",
    "                if self.nc > 1:  # cls loss (only if multiple classes)\n",
    "                    t = torch.full_like(ps[:, 5:], self.cn, device=device)  # targets\n",
    "                    t[range(n), tcls[i]] = self.cp\n",
    "                    lcls += self.BCEcls(ps[:, 5:], t)  # BCE\n",
    "                    \n",
    "            obji = self.BCEobj(pi[..., 4], tobj)\n",
    "            lobj += obji * self.balance[i]  # obj loss\n",
    "            if self.autobalance:\n",
    "                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n",
    "                \n",
    "        if self.autobalance:\n",
    "            self.balance = [x / self.balance[self.ssi] for x in self.balance]\n",
    "        lbox *= config.box_loss_gain\n",
    "        lobj *= config.obj_loss_gain\n",
    "        lcls *= config.cls_loss_gain\n",
    "        bs = tobj.shape[0]  # batch size\n",
    "        \n",
    "        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()\n",
    "    \n",
    "    def build_targets(self, p, targets):\n",
    "        # Build targets for compute_loss()\n",
    "        na, nt = self.na, targets.shape[0]  # number of anchors, targets\n",
    "        tcls, tbox, indices, anch = [], [], [], []\n",
    "        gain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\n",
    "        ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)\n",
    "        targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)  # append anchor indices\n",
    "        \n",
    "        g = 0.5  # bias\n",
    "        off = torch.tensor([[0, 0],\n",
    "                           [1, 0], [0, 1], [-1, 0], [0, -1],  # j,k,l,m\n",
    "                           ], device=targets.device).float() * g  # offsets\n",
    "        \n",
    "        for i in range(self.nl):\n",
    "            anchors = self.anchors[i]\n",
    "            gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "            \n",
    "            # Match targets to anchors\n",
    "            t = targets * gain\n",
    "            if nt:\n",
    "                # Matches\n",
    "                r = t[:, :, 4:6] / anchors[:, None]  # wh ratio\n",
    "                j = torch.max(r, 1 / r).max(2)[0] < config.anchor_t  # compare\n",
    "                t = t[j]  # filter\n",
    "                \n",
    "                # Offsets\n",
    "                gxy = t[:, 2:4]  # grid xy\n",
    "                gxi = gain[[2, 3]] - gxy  # inverse\n",
    "                j, k = ((gxy % 1 < g) & (gxy > 1)).T\n",
    "                l, m = ((gxi % 1 < g) & (gxi > 1)).T\n",
    "                j = torch.stack((torch.ones_like(j), j, k, l, m))\n",
    "                t = t.repeat((5, 1, 1))[j]\n",
    "                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n",
    "            else:\n",
    "                t = targets[0]\n",
    "                offsets = 0\n",
    "                \n",
    "            # Define\n",
    "            b, c = t[:, :2].long().T  # image, class\n",
    "            gxy = t[:, 2:4]  # grid xy\n",
    "            gwh = t[:, 4:6]  # grid wh\n",
    "            gij = (gxy - offsets).long()\n",
    "            gi, gj = gij.T  # grid xy indices\n",
    "            \n",
    "            # Append\n",
    "            a = t[:, 6].long()  # anchor indices\n",
    "            indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices\n",
    "            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "            anch.append(anchors[a])  # anchors\n",
    "            tcls.append(c)  # class\n",
    "            \n",
    "        return tcls, tbox, indices, anch\n",
    "\n",
    "def smooth_BCE(eps=0.1):\n",
    "    \"\"\"Label smoothing BCE targets\"\"\"\n",
    "    return 1.0 - 0.5 * eps, 0.5 * eps\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss implementation\"\"\"\n",
    "    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = loss_fcn.reduction\n",
    "        self.loss_fcn.reduction = 'none'  # required to apply FL to each element\n",
    "        \n",
    "    def forward(self, pred, true):\n",
    "        loss = self.loss_fcn(pred, true)\n",
    "        pred_prob = torch.sigmoid(pred)  # prob from logits\n",
    "        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)\n",
    "        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n",
    "        modulating_factor = (1.0 - p_t) ** self.gamma\n",
    "        loss *= alpha_factor * modulating_factor\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
    "    \"\"\"Calculate IoU between boxes\"\"\"\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "        \n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "            \n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "    \n",
    "    iou = inter / union\n",
    "    if CIoU or DIoU or GIoU:\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
    "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
    "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "            return iou - rho2 / c2  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "    return iou  # IoU\n",
    "\n",
    "print(\"Training configuration and loss functions defined!\")\n",
    "print(f\"Dataset: {config.dataset}\")\n",
    "print(f\"Classes: {config.nc}\")  \n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Image size: {config.img_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1c4b0",
   "metadata": {},
   "source": [
    "## 9. Performance Evaluation and Visualization\n",
    "\n",
    "Implement evaluation metrics and model analysis tools for comprehensive performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAnalyzer:\n",
    "    \"\"\"Comprehensive model analysis and evaluation tools\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total and trainable parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'total_parameters_M': total_params / 1e6,\n",
    "            'trainable_parameters_M': trainable_params / 1e6\n",
    "        }\n",
    "    \n",
    "    def analyze_model_size(self):\n",
    "        \"\"\"Analyze model memory usage\"\"\"\n",
    "        param_size = 0\n",
    "        for param in self.model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "        buffer_size = 0\n",
    "        for buffer in self.model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "            \n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        \n",
    "        return {\n",
    "            'parameter_size_MB': param_size / 1024**2,\n",
    "            'buffer_size_MB': buffer_size / 1024**2,\n",
    "            'total_size_MB': size_all_mb\n",
    "        }\n",
    "    \n",
    "    def profile_inference_speed(self, input_size=(1, 3, 640, 640), num_runs=100, warmup_runs=10):\n",
    "        \"\"\"Profile inference speed\"\"\"\n",
    "        device = next(self.model.parameters()).device\n",
    "        dummy_input = torch.randn(*input_size, device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(warmup_runs):\n",
    "                _ = self.model(dummy_input)\n",
    "        \n",
    "        # Timing\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_runs):\n",
    "                _ = self.model(dummy_input)\n",
    "                \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / num_runs\n",
    "        fps = 1 / avg_time\n",
    "        \n",
    "        return {\n",
    "            'average_inference_time_ms': avg_time * 1000,\n",
    "            'fps': fps,\n",
    "            'device': str(device)\n",
    "        }\n",
    "    \n",
    "    def visualize_architecture(self):\n",
    "        \"\"\"Visualize model architecture\"\"\"\n",
    "        def count_layers_by_type(model):\n",
    "            layer_counts = {}\n",
    "            for name, module in model.named_modules():\n",
    "                module_type = type(module).__name__\n",
    "                if module_type != 'LEAFYOLO' and not name == '':  # Skip root module\n",
    "                    layer_counts[module_type] = layer_counts.get(module_type, 0) + 1\n",
    "            return layer_counts\n",
    "        \n",
    "        layer_counts = count_layers_by_type(self.model)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Layer distribution pie chart\n",
    "        plt.subplot(2, 2, 1)\n",
    "        labels = list(layer_counts.keys())\n",
    "        sizes = list(layer_counts.values())\n",
    "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Layer Distribution')\n",
    "        \n",
    "        # Parameter distribution bar chart\n",
    "        plt.subplot(2, 2, 2)\n",
    "        param_counts = {}\n",
    "        for name, module in self.model.named_modules():\n",
    "            module_type = type(module).__name__\n",
    "            if hasattr(module, 'parameters') and module_type != 'LEAFYOLO':\n",
    "                param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "                if param_count > 0:\n",
    "                    param_counts[module_type] = param_counts.get(module_type, 0) + param_count\n",
    "        \n",
    "        if param_counts:\n",
    "            modules = list(param_counts.keys())\n",
    "            params = [param_counts[m] / 1000 for m in modules]  # Convert to thousands\n",
    "            plt.bar(modules, params)\n",
    "            plt.title('Parameters by Module Type (K)')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Model summary text\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Get model info\n",
    "        param_info = self.count_parameters()\n",
    "        size_info = self.analyze_model_size()\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        LEAF-YOLO Model Summary\n",
    "        ========================\n",
    "        \n",
    "        Architecture: {self.model.get_model_info()['model_type']}\n",
    "        Width Multiple: {self.model.width_multiple}\n",
    "        \n",
    "        Parameters:\n",
    "        - Total: {param_info['total_parameters_M']:.2f}M\n",
    "        - Trainable: {param_info['trainable_parameters_M']:.2f}M\n",
    "        \n",
    "        Memory Usage:\n",
    "        - Parameters: {size_info['parameter_size_MB']:.2f} MB\n",
    "        - Buffers: {size_info['buffer_size_MB']:.2f} MB\n",
    "        - Total: {size_info['total_size_MB']:.2f} MB\n",
    "        \n",
    "        Target Dataset: VisDrone2019-DET (10 classes)\n",
    "        Input Size: 640x640x3\n",
    "        Output Scales: P2, P3, P4, P5 (4 scales)\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center', \n",
    "                fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "class VisDroneMetrics:\n",
    "    \"\"\"Evaluation metrics specifically for VisDrone dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, class_names):\n",
    "        self.class_names = class_names\n",
    "        self.nc = len(class_names)\n",
    "        \n",
    "    def compute_ap(self, tp, conf, pred_cls, target_cls):\n",
    "        \"\"\"Compute Average Precision\"\"\"\n",
    "        # Sort by objectness\n",
    "        i = np.argsort(-conf)\n",
    "        tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "        \n",
    "        # Find unique classes\n",
    "        unique_classes = np.unique(target_cls)\n",
    "        \n",
    "        # Create Precision-Recall curve and compute AP for each class\n",
    "        pr_score = 0.1  # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n",
    "        \n",
    "        # number of classes and number of detections\n",
    "        nc = len(unique_classes)\n",
    "        ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "        \n",
    "        for ci, c in enumerate(unique_classes):\n",
    "            i = pred_cls == c\n",
    "            n_l = (target_cls == c).sum()  # number of labels\n",
    "            n_p = i.sum()  # number of predictions\n",
    "            \n",
    "            if n_p == 0 or n_l == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # Accumulate FPs and TPs\n",
    "                fpc = (1 - tp[i]).cumsum(0)\n",
    "                tpc = tp[i].cumsum(0)\n",
    "                \n",
    "                # Recall\n",
    "                recall = tpc / (n_l + 1e-16)  # recall curve\n",
    "                r[ci] = np.interp(-pr_score, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "                \n",
    "                # Precision\n",
    "                precision = tpc / (tpc + fpc)  # precision curve\n",
    "                p[ci] = np.interp(-pr_score, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "                \n",
    "                # AP from recall-precision curve\n",
    "                for j in range(tp.shape[1]):\n",
    "                    ap[ci, j], mpre, mrec = self.compute_ap_per_class(recall[:, j], precision[:, j])\n",
    "                    \n",
    "        return ap, p, r\n",
    "    \n",
    "    def compute_ap_per_class(self, recall, precision):\n",
    "        \"\"\"Compute AP for a single class\"\"\"\n",
    "        # Append sentinel values at beginning and end\n",
    "        mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "        mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "        \n",
    "        # Compute precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "            \n",
    "        # Look for recall value changes\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "        \n",
    "        # Sum Recall * Precision\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "        \n",
    "        return ap, mpre, mrec\n",
    "    \n",
    "    def print_results(self, ap, p, r, class_names=None):\n",
    "        \"\"\"Print evaluation results\"\"\"\n",
    "        if class_names is None:\n",
    "            class_names = self.class_names\n",
    "            \n",
    "        # Print results\n",
    "        pf = '%20s' + '%12s' * 6  # print format\n",
    "        print(pf % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95'))\n",
    "        print('-' * 86)\n",
    "        \n",
    "        # Per-class results\n",
    "        for i, c in enumerate(class_names):\n",
    "            print(pf % (c, 0, 0, p[i, 0], r[i, 0], ap[i, 0], ap[i].mean()))\n",
    "            \n",
    "        # Overall results\n",
    "        print(pf % ('all', 0, 0, p.mean(), r.mean(), ap[:, 0].mean(), ap.mean()))\n",
    "\n",
    "# Performance Analysis\n",
    "print(\"Performing comprehensive model analysis...\")\n",
    "\n",
    "# Analyze nano model\n",
    "analyzer_nano = ModelAnalyzer(model_nano)\n",
    "analyzer_standard = ModelAnalyzer(model_standard)\n",
    "\n",
    "# Parameter analysis\n",
    "nano_params = analyzer_nano.count_parameters()\n",
    "standard_params = analyzer_standard.count_parameters()\n",
    "\n",
    "print(\"\\\\nParameter Analysis:\")\n",
    "print(f\"LEAF-YOLO-N: {nano_params['total_parameters_M']:.2f}M parameters\")\n",
    "print(f\"LEAF-YOLO Standard: {standard_params['total_parameters_M']:.2f}M parameters\")\n",
    "\n",
    "# Model size analysis\n",
    "nano_size = analyzer_nano.analyze_model_size()\n",
    "standard_size = analyzer_standard.analyze_model_size()\n",
    "\n",
    "print(\"\\\\nModel Size Analysis:\")\n",
    "print(f\"LEAF-YOLO-N: {nano_size['total_size_MB']:.2f} MB\")\n",
    "print(f\"LEAF-YOLO Standard: {standard_size['total_size_MB']:.2f} MB\")\n",
    "\n",
    "# Inference speed (if CUDA available)\n",
    "if torch.cuda.is_available():\n",
    "    model_nano = model_nano.cuda()\n",
    "    nano_speed = analyzer_nano.profile_inference_speed(num_runs=50)\n",
    "    print(f\"\\\\nInference Speed (LEAF-YOLO-N on GPU):\")\n",
    "    print(f\"Average time: {nano_speed['average_inference_time_ms']:.2f} ms\")\n",
    "    print(f\"FPS: {nano_speed['fps']:.1f}\")\n",
    "else:\n",
    "    print(\"\\\\nCUDA not available - skipping GPU inference speed test\")\n",
    "\n",
    "# Visualize architecture\n",
    "print(\"\\\\nGenerating model architecture visualization...\")\n",
    "analyzer_nano.visualize_architecture()\n",
    "\n",
    "# Expected VisDrone performance (based on paper)\n",
    "print(\"\\\\nExpected VisDrone Performance:\")\n",
    "print(\"LEAF-YOLO-N:\")\n",
    "print(\"  - mAP@50:95: 21.9%\")\n",
    "print(\"  - mAP@50: 39.7%\") \n",
    "print(\"  - AP_S (small): 14.0%\")\n",
    "print(\"  - Jetson AGX Xavier: 56 FPS\")\n",
    "print(\"\\\\nLEAF-YOLO Standard:\")\n",
    "print(\"  - mAP@50:95: 28.2%\")\n",
    "print(\"  - mAP@50: 48.3%\")\n",
    "print(\"  - AP_S (small): 20.0%\") \n",
    "print(\"  - Jetson AGX Xavier: 32 FPS\")\n",
    "\n",
    "print(\"\\\\nModel analysis completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
